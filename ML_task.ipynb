{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "# Sets up the environment by importing \n",
    "# pandas, numpy, matplotlib.\n",
    "\n",
    "### YOU MAY ADD ADDITIONAL IMPORTS IF YOU WISH\n",
    "### HERE OR IN ANY OF THE CELLS BELOW\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import scipy.stats as ss\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 1 - Beta Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will develop a maximum likelihood regression model where both the target values and the predictions are strictly between between 0 and 1. This scenario arises often when data represent a percentage or proportion, but where the \"counts\" that make up the proportion are unknown. For example, measurements of blood oxygen saturation range from 0 (completely desaturated) to 1 (completely saturated.)\n",
    "\n",
    "There are four main parts to this task.\n",
    "\n",
    "1. Choose and implement an appropriate function that maps inputs $X$ and parameters $b$ to a predictive mean $\\mu$.\n",
    "\n",
    "2. Define an appropriate log likelihood of the data given parameters.\n",
    "\n",
    "3. Define the negative log likelihood function for beta regression.\n",
    "\n",
    "4. Apply your regression model to an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 1 - Mean Function \n",
    "\n",
    "Complete the definition of the function below. Your function must take an $n \\times m$ matrix $X$ and an $m \\times 1$ vector of parameters and produce a prediction for each row of $X$ that is between 0 and 1. You can use a built-in function or define \"from scratch\"; both are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26894142],\n",
       "       [0.11920292],\n",
       "       [0.04742587]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(X,b):\n",
    "    # Put your code below\n",
    "    # Logistic regression also needs to make predictions between 0 and 1; \n",
    "    # we'll just use the sigmoid function like we do for LR\n",
    "    # Your code below ==================\n",
    "    eta = np.dot(X,b)\n",
    "    predictions = 1.0 / (1+np.exp(-eta))\n",
    "    # ==================================\n",
    "    return predictions\n",
    "    \n",
    "# Example:    \n",
    "\n",
    "X = np.array([[1,2],[1,3],[1,4]])\n",
    "b = np.array([[1],[-1]])\n",
    "\n",
    "predict(X,b)\n",
    "\n",
    "# Should produce:\n",
    "# array([[0.26894142],\n",
    "#        [0.11920292],\n",
    "#        [0.04742587]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 2 - Likelihood Function \n",
    "\n",
    "Your next task is to write a negative log likelihood function for this regression model. We will use the \"beta distribution\" to define the likelihood, because it is a continuous distribution that ranges from 0 to 1.\n",
    "\n",
    "The standard beta distribution takes two parameters, $a$ and $b$. The larger $a$ is, the more the distribution is \"pushed\" toward 1, and the larger $b$ is, the more it is \"pushed\" toward 0. The following code will let you see how this works; you can try different $a$ and $b$ if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f893d6b6670>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code is just so you can see what the beta distribution looks like.\n",
    "a = 8\n",
    "b = 5\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(0,1,100)\n",
    "ax.plot(x, ss.beta.pdf(x, a, b), 'r-', lw=5, alpha=0.6, label='beta pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in regression we typically model the mean as a function of inputs $X$, it is more convenient to parameterize the beta distribution in terms of the mean $\\mu \\in (0,1)$ and a \"concentration\" $\\phi > 0$, where\n",
    "\n",
    "$$a = \\mu \\cdot \\phi$$\n",
    "$$b = (1 - \\mu) \\cdot \\phi$$\n",
    "\n",
    "Implement the beta **log likelihood** function below, which takes a vector of data $y$, a vector of means $\\mu$, and a scalar (single number) concentration $\\phi$, and produces the log likelihood of $y$, assuming each element comes from a beta distribution with parameters given by the corresponding element in $\\mu$ and concentration $\\phi$.\n",
    "\n",
    "BIG HINT 1: Recall from the notes that the log likelihood of a single observation of a continuous random variable given specified parameters is the log of the probability density function for that random variable evaluated at the observation.\n",
    "\n",
    "BIG HINT 2: You can find useful helper functions here https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html which you can access in the form 'ss.beta.functionname(...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.719823849145598"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def betaloglik(y,mu,phi):\n",
    "    #print(f\"y,mu,phi: {y},{mu},{phi}\")\n",
    "    # Your code below ================\n",
    "    a = mu*phi # Get a from mu and phi using the formula\n",
    "    b = (1-mu)*phi # Get b from mu and phi using the formula\n",
    "    #print(f\"y:{y}, a:{a}, b:{b}\")\n",
    "    #print(f\"logprobs: {ss.beta.logpdf(y,a,b)}\")\n",
    "    return ss.beta.logpdf(y, a, b).sum() # Call the special logpdf function. \n",
    "    # ================================\n",
    "    # (Taking log of beta.pdf probably also works fine but will be less precise\n",
    "    #  than a specially-made function, and may not optimize as well. Full marks though.)\n",
    "    \n",
    "# Example:\n",
    "betaloglik(np.array([0.2,0.3,0.4,0.5,0.6]), np.array([0.2,0.3,0.4,0.5,0.6]), 100)\n",
    "# should give\n",
    "# 10.719823849145598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 3 - Regression Negative Log Likelihood \n",
    "Now you will define the regression negative log likelihood, where the values for $\\mu$ come from regression predictions. This will make it a bit tricky because the function must accept $\\phi$ and $b$ together as one parameter so that we can later use the minimize function. Skeleton code and test code is provided to take care of this; just fill in the missing pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) (1, 3) (1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.507492549644741"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def betaregnegloglik(theta,X,y):\n",
    "    # theta contains phi followed by beta\n",
    "    phi = theta[0]\n",
    "    beta = theta[1:]\n",
    "    # Your code below =======\n",
    "    mu = predict(X, beta).T # Use our predict function\n",
    "    print(y.shape, mu.shape, phi.shape)\n",
    "    return -1*betaloglik(y, mu, phi) # Negate the beta log likelihood\n",
    "    # =======================\n",
    "\n",
    "X = np.array([[1,2],[1,3],[1,4]])\n",
    "b = np.array([[1],[-1]])\n",
    "y = np.array([[0.2],[0.4],[0.6]])\n",
    "\n",
    "phi = np.array([[10]]) # Makes phi a 1 by 1 array\n",
    "testtheta = np.r_[phi,b] # Stack phi on top of b to get theta\n",
    "betaregnegloglik(testtheta,X,np.array([0.2,0.4,0.6])) # Compute\n",
    "# Should produce 16.50271105637038 # WRONG. Shape issue.\n",
    "# Should produce 7.507492549644741"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 4 - Maximum Likelihood Beta Regression \n",
    "\n",
    "This example comes from the prominent statistical analysis software *stata*. The manual page for beta regression in stata is here:\n",
    "\n",
    "[ https://www.stata.com/manuals/rbetareg.pdf ]\n",
    "\n",
    "The following example uses synthetic (i.e. made-up) data about the pass rate at different schools.\n",
    "\n",
    "The variable to predict is the school-wide pass rate given in the *prate* variable, which must be between 0 and 1.\n",
    "\n",
    "The predictive variables are *freemeals* which denotes the proportion of students receiving free meals at school, *pdonations* which denotes the level of donations received by the school, and *summer* which indicates whether or not the school puts on a summer instruction program for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freemeals</th>\n",
       "      <th>pdonations</th>\n",
       "      <th>prate</th>\n",
       "      <th>summer_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.400399</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>0.841196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.189950</td>\n",
       "      <td>1.095856</td>\n",
       "      <td>0.891308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.427926</td>\n",
       "      <td>1.197783</td>\n",
       "      <td>0.898432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.273827</td>\n",
       "      <td>1.141752</td>\n",
       "      <td>0.858955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423951</td>\n",
       "      <td>2.958525</td>\n",
       "      <td>0.958682</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   freemeals  pdonations     prate  summer_yes\n",
       "0   0.400399    1.798023  0.841196           1\n",
       "1   0.189950    1.095856  0.891308           1\n",
       "2   0.427926    1.197783  0.898432           1\n",
       "3   0.273827    1.141752  0.858955           1\n",
       "4   0.423951    2.958525  0.958682           1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_stata(\"sprogram.dta\")\n",
    "df = pd.get_dummies(df, drop_first = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in the cell below to find the maximum likelihood coefficients for beta regression. Include an intercept. Use the 'Nelder-Mead' method, which does not require a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n",
      "(1000,) (1000,) ()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.17499393, -0.45635901,  0.04496522,  0.55600845]),\n",
       " 10.755643379822022)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function definition - 10 pts\n",
    "def betaregmaximumlikelihood(X,y):\n",
    "    phistart = np.ones((1,1))\n",
    "    bstart = np.zeros((X.shape[1],1))\n",
    "    thetastart = np.r_[phistart, bstart] # Stack one on top of the other\n",
    "    \n",
    "    # Your code below ============\n",
    "    # Call the minimize function, similar to first assignments\n",
    "    optimization = minimize(betaregnegloglik, x0=thetastart, \n",
    "                            args = (X,y), method = 'Nelder-Mead')\n",
    "    #print(f\"minimum: {optimization.fun}\")\n",
    "    results = optimization.x # Get the \"theta\" vector back\n",
    "    phi = results[0] # phi is first element\n",
    "    b = results[1:] # b is rest\n",
    "    # ===========================\n",
    "    \n",
    "    return (b,phi)\n",
    "\n",
    "(n,m) = df.shape\n",
    "X = np.c_[np.ones((n,1)),df.drop('prate', axis='columns').values]\n",
    "y = df.prate\n",
    "\n",
    "betaregmaximumlikelihood(X,y)\n",
    "\n",
    "# Should produce\n",
    "# (array([ 1.17499393, -0.45635901,  0.04496522,  0.55600845]), 10.755643379822022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((n,1)),df.drop('prate', axis='columns').values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 5 - Interpreting the Beta Regression Model \n",
    "\n",
    "Based on the regression coefficients, for each of the input variables in the dataset, explain how a change in its value is associated with a change in the estimated proportion of students who attain a passing grade. (You can just explain the direction of the change rather than the magnitude.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer below:\n",
    "\n",
    "When freemeals increases, estimated proportion of those with a passing grade decreases (negative coef)\n",
    "\n",
    "When pdonations increases estimated proportion of those with a passing grade increases (positive coef)\n",
    "\n",
    "When summer_yes increases (or when it is 1 compared to when it is 0), estimated proportion of those with a passing grade increases (positive coef.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['freemeals', 'pdonations', 'prate', 'summer_yes'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns # Did this to remind myself of order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 2 - Classification and Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set \n",
    "The Child Health and Development Studies investigate a range of topics. One study considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area.\n",
    "\n",
    "The data frame contains the following data, where each row represents a baby:\n",
    "\n",
    "- bwt: birth weight (ounces)\n",
    "- gestation: length of pregnancy (days)\n",
    "- parity: 0 if mom's first baby, 1 if not mom's first baby\n",
    "- age: mom's age (years)\n",
    "- height: mom's height (inches)\n",
    "- weight: mom's weight (pounds)\n",
    "- smoke: 0 if mom is nonsmoker, 1 if mom is smoker\n",
    "\n",
    "Note that babies that are born with a gestation of less than 37 weeks (259 days) are considered pre-term and are at a higher risk for health complications.\n",
    "\n",
    "Overall this task will focus on building a good predictive model whether a baby will be pre-term or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data set and drops the case number, as well as any Missing observations: \n",
    "df = pd.read_csv('babies.csv').drop('case',axis='columns').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 1 - Warm up \n",
    "In the cell below, write code that does the following:\n",
    "- Prints the number of observations in the dataset\n",
    "- Prints the number of variables in the dataset (all variables regardless of whether they are a predictor or label or neither)\n",
    "- Adds a new variable to the dataset called 'preterm' that is 1 if the baby is pre-term and 0 otherwise\n",
    "- Prints the number of pre-term babies in the dataset\n",
    "- Prints the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174\n",
      "7\n",
      "96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bwt</th>\n",
       "      <th>gestation</th>\n",
       "      <th>parity</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>smoke</th>\n",
       "      <th>preterm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>284.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128</td>\n",
       "      <td>279.0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bwt  gestation  parity   age  height  weight  smoke  preterm\n",
       "0  120      284.0       0  27.0    62.0   100.0    0.0        0\n",
       "1  113      282.0       0  33.0    64.0   135.0    0.0        0\n",
       "2  128      279.0       0  28.0    64.0   115.0    1.0        0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Warm-up code ==================\n",
    "\n",
    "# Number of obs\n",
    "print(df.shape[0])\n",
    "\n",
    "# Number of vars\n",
    "print(df.shape[1])\n",
    "\n",
    "# Add preterm\n",
    "df['preterm'] = df.gestation.apply(lambda x: 1 if x<259 else 0)\n",
    "\n",
    "# Num preterms\n",
    "print(df.preterm.sum())\n",
    "\n",
    "# First few rows\n",
    "df.head(3)\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 2 - Logistic Regression Model \n",
    "\n",
    "Your next task is to build a model that predicts whether a baby will be pre-term or not using the characteristics of the mother (assuming they are measured prior to birth.) In the cell below, estimate a logistic regression model for pre-term status given these characteristics and print out its coefficients and intercept. Do not use a penalty for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35512466  0.00058173 -0.04752664  0.00488872  0.13200691]]\n",
      "[0.00343994]\n"
     ]
    }
   ],
   "source": [
    "# Code for building a logistic regression model\n",
    "\n",
    "# Your code here =======================\n",
    "model = LogisticRegression(penalty='none', max_iter = 10000)\n",
    "\n",
    "#X = df.loc[:, ['parity','age','height','weight','smoke']]\n",
    "X = df.drop(['preterm','gestation','bwt'], axis='columns').values\n",
    "\n",
    "\n",
    "y = df.preterm.values\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.coef_)\n",
    "print(model.intercept_)\n",
    "# ======================================\n",
    "\n",
    "\n",
    "# Actual Coefficients: [[-0.35512466  0.00058173 -0.04752664  0.00488872  0.13200691]]\n",
    "# Actual Intercept: [0.00343994]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 3 - In-sample Evaluation \n",
    "\n",
    "In the cell below, compute the accuracy and the AUROC for your classifier using the training data and print these out. Then explain why the observed accuracy is much higher than the AUROC score in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9182282793867121\n",
      "AUROC Score 0.5654520330859616\n"
     ]
    }
   ],
   "source": [
    "# Code for computing accuracy and AUROC \"in-sample\", that is, on the training data.\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Your code =======================\n",
    "#Label predictions for accuracy\n",
    "ypred = model.predict(X)\n",
    "#Probability predictions for AUROC\n",
    "ypred_prob = model.predict_proba(X)[:,1]\n",
    "\n",
    "print('Accuracy', accuracy_score(y, ypred))\n",
    "print('AUROC Score', roc_auc_score(y, ypred_prob))\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2P3 Written answer: Explain in this cell why the observed accuracy is much higher than the AUROC score in this case.\n",
    "\n",
    "# Delete below\n",
    "\n",
    "This question is ill-defined IMO.  If by \"why\" they are different you mean \"why do they not return the same value\" then the answer is because they are different metrics and are computed differently.\n",
    "\n",
    "If by \"why\" you mean \"what causes the discrepency\" then we can note that only 96 of the 1174 observations have a preterm pregnancy, and so simply guessing 0 can yield fairly decent accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 4 - Out-of-sample Evaluation \n",
    "In the cell below, compute the accuracy and the AUROC for your classifier using 10-fold stratified cross-validation and print these out. Then explain:\n",
    " 1) Why stratified cross-validation is appropriate for this dataset and\n",
    " 2) Any differences you see between these CV-based estimates from your in-sample estimates from Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9182384470520064\n",
      "AUROC 0.513917349332718\n"
     ]
    }
   ],
   "source": [
    "# Code for computing accuracy using stratified K-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "# Delete below\n",
    "cver = StratifiedKFold(n_splits = 10)\n",
    "#cver = LeaveOneOut()\n",
    "LogFF = LogisticRegression(penalty='l2', solver='saga', tol=0.001, \n",
    "                           max_iter=10000, C=0.006)\n",
    "\n",
    "accuracy_cv = cross_val_score(model, X, y, scoring = 'accuracy', cv = cver)\n",
    "#auroc_cv = cross_val_score(model, X, y, scoring = 'roc_auc', cv = cver)\n",
    "auroc_cv = cross_val_score(model, X, y, scoring = make_scorer(roc_auc_score, needs_proba=True), cv = cver)\n",
    "\n",
    "\n",
    "print('Accuracy', accuracy_cv.mean())\n",
    "print('AUROC', auroc_cv.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2P4 Written answer\n",
    "\n",
    "1. Explain why stratified cross-validation is appropriate for this dataset\n",
    "\n",
    "Stratified cross-validation makes sure that there is the same proportion of + and - examples in each fold. This is important with so much imbalance, because k-fold cross validation can create folds by accident that have no + examples.\n",
    "\n",
    "2. Explain why these estimated values are different from those in Part 3.\n",
    "\n",
    "These are out-of-sample estimates using cross-validation, rather than in-sample esitmates on the training data. AUROC is probably lower here because in-sample estimates are optimistic. Accuracy is almost exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 5 - Discussing Evaluation \n",
    "\n",
    "There are many ways to evaluate a classifier. For each of the four that you just computed, (training accuracy, training AUROC, CV accuracy, CV AUROC) explain in a sentence or two whether it is useful for assessing the generalization performance of your learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written answer: Explain here.\n",
    "\n",
    "1. Training accuracy - Training accuracy is not very useful because it can be too optimistic as an estimate of generalization error and because there is a big class imbalance.\n",
    "\n",
    "2. Training AUROC - Training AUROC is better than training accuracy, but it is still an optimistic estimate.\n",
    "\n",
    "3. CV accuracy - CV accuracy shouldn't be optimistic but it is still not very useful because of the class imbalance.\n",
    "\n",
    "4. CV AUROC - CV AUROC is the most useful because it is not optimistically biased and is more informative than accuracy when classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 3 - Model Selection, Regularization \n",
    "\n",
    "In this section we will use the same data but we will predict **birth weight** from the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 1 - Linear Regression \n",
    "\n",
    "In the cell below, provide code to use 10 fold cross-validation to assess the performance of a linear regression model to predict birth weight from the other variables in terms of its root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.856957571456146"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code here for linear model, 10-fold CV.\n",
    "\n",
    "# Your code =============\n",
    "\n",
    "linreg_model = LinearRegression()\n",
    "\n",
    "X = df.loc[:, ['gestation','parity','age','height','weight','smoke']]\n",
    "\n",
    "y = df.bwt.values\n",
    "\n",
    "rmse_cv = np.sqrt(-1*cross_val_score(linreg_model, X, y, cv = 10, scoring = 'neg_mean_squared_error'))\n",
    "\n",
    "rmse_cv.mean()\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 2 - Linear Regression, Complex Model \n",
    "Use the following polynomial feature expansion pipeline component, called `preprocess`, to build a linear regression model to predict the birth weight (bwt). Read through the comments to make sure you understand what is happening. You do not need to modify any of this code.\n",
    "\n",
    "**In the cell after this giant one full of code** provide code to use the preprocessor to build a linear regression model to predict birthweight based on the transformed data, and evaluate it using root mean squared error from 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30609815 -0.03925456 -0.81163687 ... -0.10499008  0.\n",
      "   0.        ]\n",
      " [ 0.18112537  0.99249577 -0.01956564 ... -0.08346669  0.\n",
      "   0.        ]\n",
      " [-0.0063338   0.13270383 -0.01956564 ... -0.08399106  0.\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.74350288  0.47662061  0.37646997 ... -0.07817898  0.\n",
      "   1.        ]\n",
      " [ 0.11863898 -1.07100488  0.37646997 ... -0.0859542   1.\n",
      "   0.        ]\n",
      " [ 1.11842122  1.85228771  0.37646997 ... -0.08348023  0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Dummies, dropping the first category\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Get all predictors\n",
    "X = df.loc[:, ['gestation','parity','age','height','weight','smoke']]\n",
    "# Get bwt - target variable\n",
    "y = df.loc[:, ['bwt']]\n",
    "\n",
    "# List categorical features (no scaling, no polynomial-ifying)\n",
    "categorical_features = ['parity','smoke']\n",
    "# List numeric (continuous) features\n",
    "numeric_features = [column for column in X.columns if all(cat_var not in column for cat_var in categorical_features)]\n",
    "\n",
    "# Collect the names of the new features that will be created by the polynomial transformer\n",
    "numeric_names = PolynomialFeatures(5, include_bias=False).fit(X.loc[:,numeric_features]).get_feature_names(numeric_features)\n",
    "all_names = numeric_names\n",
    "# Add on the categorical feature names\n",
    "all_names.extend(categorical_features)\n",
    "# This holds all the feature names in the new space\n",
    "all_names = np.asarray(all_names)\n",
    "\n",
    "# This transformer will scale, then transform to polynomial features, then re-scale!\n",
    "# To be applied to continuous variables only.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(5, include_bias=False)),\n",
    "    ('scaler2', StandardScaler())])\n",
    "\n",
    "# This transformer pretty much does nothing. It's for the categorical variables.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Make a column transformer with two transformers, one for numeric and one for categorical.\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# You can now use the \"preprocess\" transformer in any pipeline you want, worry-free. \n",
    "# Always apply it to the original data X.\n",
    "\n",
    "# Example:\n",
    "Xtransform = preprocess.fit_transform(X)\n",
    "print(Xtransform)\n",
    "Xtransformdf = pd.DataFrame(Xtransform)\n",
    "Xtransformdf.columns = all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code above may have failed for people with old versions of sklearn - I am not sure\n",
    "sk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.55529290299085"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code here for higher-dimensional model, 10-fold CV, report RMSE\n",
    "\n",
    "# If you use a pipeline, it should have just two steps, the preprocess step (defined above) and the regression step.\n",
    "\n",
    "# Your code ================\n",
    "linreg_complex = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "rmse_cv = np.sqrt(-1*cross_val_score(linreg_complex, X, y, cv = 10, scoring = 'neg_mean_squared_error'))\n",
    "\n",
    "rmse_cv.mean()\n",
    "# ========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 3 - Feature selection with the LASSO \n",
    "\n",
    "Based on the training set alone, use 10-fold cross-validation to determine the best value for an L1 penalty (i.e., the LASSO.)\n",
    "\n",
    "Vary the penalty coefficient alpha from 0.1 to 1.0 in 20 steps. Make a scatter plot of the cross-validation-based RMSE against the the LASSO regularization parameter. Print out the regularization parameter value that you think is best, and print out the CV-based root mean squared error associated with that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     16.322856\n",
      "1     16.092544\n",
      "2     16.031685\n",
      "3     15.984414\n",
      "4     15.954593\n",
      "5     15.941185\n",
      "6     15.943613\n",
      "7     15.963597\n",
      "8     15.999464\n",
      "9     16.046271\n",
      "10    16.107992\n",
      "11    16.175202\n",
      "12    16.248043\n",
      "13    16.263759\n",
      "14    16.273299\n",
      "15    16.286056\n",
      "16    16.300656\n",
      "17    16.321617\n",
      "18    16.341449\n",
      "19    16.363343\n",
      "Name: mean_test_score, dtype: float64\n",
      "My chosen regularization value is {'las__alpha': 0.33684210526315794}\n",
      "My best RMSE score is 15.94118451077394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linglinglin/opt/anaconda3/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code here for LASSO feature selection\n",
    "from sklearn.linear_model import Lasso\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "# Compute scores for different parameter values ========\n",
    "alpha = np.linspace(0.1,1,20)\n",
    "param_grid = {'las__alpha': alpha}\n",
    "\n",
    "lassomodel = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('las', Lasso(fit_intercept=True))\n",
    "])\n",
    "\n",
    "gscv = GridSearchCV(lassomodel, param_grid=param_grid, cv = 10, scoring = 'neg_mean_squared_error')\n",
    "gscv.fit(X,y)\n",
    "# ==========================================\n",
    "\n",
    "# Code to extract and plot lambda values and rmse values =====\n",
    "cv_results = pd.DataFrame(gscv.cv_results_)\n",
    "lam = cv_results.param_las__alpha.values\n",
    "rmse = np.sqrt(-1*cv_results.mean_test_score)\n",
    "print(rmse)\n",
    "# ==============\n",
    "\n",
    "# Plot RMSE against regularization parameter (fill in)\n",
    "myLambdas = lam # Fill in\n",
    "myRMSEs = rmse # Fill in\n",
    "sns.scatterplot(myLambdas, myRMSEs)\n",
    "\n",
    "# Print out your chosen regularization parameter value (fill in)\n",
    "mylam = gscv.best_params_ # Fill in\n",
    "print(f\"My chosen regularization value is {mylam}\")\n",
    "\n",
    "# Print out the RMSE for the model with the chosen lambda value (fill in)\n",
    "myRMSE = np.sqrt(-gscv.best_score_) # Fill in\n",
    "print(f\"My best RMSE score is {myRMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 4: Interpreting Results \n",
    "Answer the questions in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many features (including the intercept) are used in your model from Part 1, Part 2, and by your \"best\" model from Part 3? If you need to run some code to find this out, you can use the cell below. (3 pts)\n",
    "1. Number of features in Part 1 model: 7 \n",
    "2. Number of features in Part 2 model: 128\n",
    "3. Number of features in best Part 3 model: 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.71331838e-01 -3.19277171e+00  3.30813654e-03  1.11517625e+00\n",
      "   4.89389571e-02 -8.15200809e+00]]\n",
      "(127,)\n",
      "27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7.27420492e+00,  0.00000000e+00,  2.66368316e+00,  6.32050349e-01,\n",
       "       -3.59717924e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  7.65839926e-02,  3.52152946e-03,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  2.94668023e-02,  2.38425443e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  4.03178132e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  6.33281875e-01,\n",
       "       -0.00000000e+00, -3.78962283e-01, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -4.36206320e-02, -0.00000000e+00,\n",
       "        7.77832278e-01,  1.16880179e+00, -3.05103856e-01, -0.00000000e+00,\n",
       "       -1.82919327e-01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  4.51030086e-02, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  2.13838222e-01, -0.00000000e+00,\n",
       "        5.88577442e-01,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -3.16068697e+00,  0.00000000e+00,  6.90464839e-01,\n",
       "        0.00000000e+00, -7.60913140e-02, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.84241613e-01, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -3.53988972e-01, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -7.25765821e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -1.50701342e+00, -6.98321728e+00])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(linreg_model.fit(X,y).coef_)\n",
    "print(gscv.best_estimator_['las'].coef_.shape)\n",
    "print((gscv.best_estimator_['las'].coef_ != 0).sum())     \n",
    "gscv.best_estimator_['las'].coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider the models you built in Part 1, Part 2, and the best model from Part 3. If you had to choose one of these models to deploy, which one would you choose? Explain your answer in 2 or 3 sentences. (2 pts)\n",
    "\n",
    "I would choose the Part 1 model because its performance is almost identical to the Part 3 model but it has fewer parameters and I think it would be more likely to generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
